# test-tender-scraper

## Настройка окружения

```bash

```

## Команды

### Создание и наполнение файла .csv/.db

Пример создания и наполнения файла с именем new_tenders 20-ю тендерами

```bash
python main.py extract --max 20 --output new_tenders.db
```

### Вывод файла .csv/.db

Пример вывода всех тендеров из файла new_tenders.db

```bash
python main.py show --output new_tenders.db
```

### Удаление файла .csv/.db или всех таких файлов в директории проекта

Пример удаления csv файла с именем new_tenders.db

```bash
python main.py delete --path new_tenders.db
```

Пример удаления всех файлов .csv и .db

```bash
python main.py delete --all 
```

### Запуск сервера раздачи json

```bash
python main.py host 
```

## Главное

Скрипт парсит сайт [rostender](https://rostender.info/extsearch) и выводит данные в консоль, также можно настроить хостинг апи /tenders

В корне лежит клиент который выводит пример json объекта-тендера

Файлы генерируются через CLI в корневую папку data

## Инструменты

Парсинг: через асинхронный httpx парсил сразу все N/20 страниц (N - число запрашиваемых тендеров) и писал в sqlite3/CSV файл, в зависимости от расширения которое получал из CLI 

CLI: реализовал через новый для себя инструмент typer(до этого работал с Argparse) но благо синтаксис и логика реализации схожа с fastapi

Database-scenario: впервые работаю с sqlite3(раньше использовал только postgreSQL) и довольно компакто и быстро реализовал функционал создания и записи в неё

Routing: Использовал классические инструменты (fastAPI + pydantic-валидатор-ORM)

## To Do

1) Из того что не заявленно в ТЗ я бы добавил фильтрацию и аггрегацию записей которые получаю в CLI.
